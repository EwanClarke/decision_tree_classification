import pandas as pd
import numpy as np
from src.algorithms.ID3.ID3 import test_decision_tree, output_decision_tree, train_decision_tree
from src.algorithms.randomForest.randomForest import construct_random_forest, test_random_forest
from src.datasetSplitting import stratified_train_test_split, stratified_k_fold_split
import argparse
from src.preprocessing import equal_frequency_data_binning
from collections import defaultdict
from src.utils import calculate_performance_metrics


def run_algorithms(algorithms, train, test, column_names, classes, classification_column_index, confidence_threshold, residual_threshold):
    decision_trees = {}
    confusion_matrices = {}
    for algorithm in algorithms:
        match algorithm:
            case "ID3":
                decision_trees[algorithm] = train_decision_tree(train, column_names, classes,classification_column_index,
                                                                confidence_threshold, residual_threshold)
                confusion_matrices[algorithm],_ = test_decision_tree(decision_trees[algorithm], test, column_names,
                                                                       classes, classification_column_index)
            case "random_forest":
                decision_trees[algorithm] = construct_random_forest(train, column_names, classes, classification_column_index,
                                                                    confidence_threshold, residual_threshold)
                confusion_matrices[algorithm],_ = test_random_forest(decision_trees[algorithm], test,
                                                                              column_names, classes, classification_column_index)
    return decision_trees, confusion_matrices


if __name__ == '__main__':
    algorithm_choices = ["ID3", "random_forest"]
    parser = argparse.ArgumentParser()
    parser.add_argument("-f", "--file", help="file name of csv dataset in the datasets folder", type=str, required=True)
    parser.add_argument("-a", "--algorithms", help="name of the algorithm(s) to run", nargs='+',
                        default=algorithm_choices[1], choices=algorithm_choices, required=True)
    parser.add_argument("-i", "--ignore", help="index(es) of columns to ignore", nargs='+', type=int)
    parser.add_argument("-bc","--binningColumns", help="index(es) of columns to bin", nargs='+', type=int)
    parser.add_argument("-b", "--bins", default=3, type=int)
    parser.add_argument("-cc","--classificationColumn", help="index of the classification column in the dataset", type=int, required=True)
    parser.add_argument("-ct", "--confidenceThreshold", help="the confidence threshold used for bounding", default=1.0, type=float)
    parser.add_argument("-rt", "--residualThreshold", help="the residual data threshold used for bounding", default=1, type=int)
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("-ts", "--trainingSplit", help="the proportion of the dataset which will be used in the training set", type=float)
    group.add_argument("--folds", help="the number of folds to use for cross validation", type=int)
    parser.add_argument("-d", "--display", help="displays the decision tree generated by the algorithm", action="store_true")
    args = parser.parse_args()
    data_pd = pd.read_csv(f"datasets/{args.file}")
    column_names = list(data_pd.keys())
    data_np = data_pd.to_numpy()

    classification_column = args.classificationColumn

    if args.binningColumns is not None:
        print(" binning ".center(40, "-"))
        for column_index in args.binningColumns:
            if not isinstance(data_np[0][column_index], str):
                column = data_np[:, column_index]
                print(f"column {column_names[column_index]}")
                data_np[:, column_index] = equal_frequency_data_binning(args.bins, column)

    if args.ignore is not None:
        print(" deleting ".center(40, "-"))
        sorted_ignore_column_indexes = sorted(args.ignore, reverse=True)
        for column_index in sorted_ignore_column_indexes:
            print(f"index data type: {column_index}")
            print(f"column {column_names[column_index]}")
            column_names.pop(column_index)
            data_np = np.delete(data_np, column_index, 1)
            if column_index < classification_column:
                classification_column -= 1

    accuracies = {algorithm: 0 for algorithm in args.algorithms}
    classes = list(set(data_np[:, classification_column]))
    if args.trainingSplit is not None:
        print(" stratified train test split ".center(40, "-"))
        train, test = stratified_train_test_split(data_np, classification_column, args.trainingSplit)
        decision_trees, confusion_matrices = run_algorithms(args.algorithms, train, test, column_names,
                                                            classes,
                                                            classification_column,
                                                            args.confidenceThreshold,
                                                            args.residualThreshold)


        if args.display:
            for algorithm in args.algorithms:
                print(f" {algorithm.replace("_", " ")} decision tree ".center(40, "-"))
                output_decision_tree(decision_trees[algorithm])
                print("")
        print("")
        for algorithm in args.algorithms:
            for i, distinct_class in enumerate(classes):
                accuracy, precision, recall, f1_score = calculate_performance_metrics(confusion_matrices[algorithm], i)
                print(f" {distinct_class} performance metrics ".center(40, "-"))
                print(f"accuracy = {accuracy}")
                print(f"precision = {precision}")
                print(f"recall = {recall}")
                print(f"f1 score = {f1_score}\n")

    elif args.folds is not None:
        print(" stratified k-fold cross validation ".center(40, "-"))
        fold_decision_trees = {}
        fold_confusion_matrices = {}

        folded_data = stratified_k_fold_split(data_np.copy(), classification_column, args.folds)
        for i in range(len(folded_data)):
            print(f"running fold {i+1}")
            test = np.array(folded_data[i])
            train = []
            for j in range(len(folded_data)):
                if j != i:
                    train += folded_data[j]
            train = np.array(train)
            fold_decision_trees[i], fold_confusion_matrices[i] = run_algorithms(args.algorithms, train, test, column_names,
                                                                                classes,
                                                                                 classification_column,
                                                                                 args.confidenceThreshold,
                                                                                 args.residualThreshold)
        performance_metrics_per_fold = []
        for i, specific_fold_confusion_matrix in enumerate(list(fold_confusion_matrices.values())):
            performance_metrics_per_fold.append({})
            for algorithm in args.algorithms:
                # print(f" {algorithm.replace("_", " ")} fold {i+1} ".center(40, "-"))
                performance_metrics_per_fold[i][algorithm] = {}
                for j, distinct_class in enumerate(classes):

                    performance_metrics_per_fold[i][algorithm][distinct_class] = list(
                        calculate_performance_metrics(specific_fold_confusion_matrix[algorithm], j))
                    # print(f" {distinct_class} performance metrics ".center(40, "_"))
                    # print(f"accuracy = {performance_metrics_per_fold[i][algorithm][distinct_class][0]}")
                    # print(f"precision = {performance_metrics_per_fold[i][algorithm][distinct_class][1]}")
                    # print(f"recall = {performance_metrics_per_fold[i][algorithm][distinct_class][2]}")
                    # print(f"f1 score = {performance_metrics_per_fold[i][algorithm][distinct_class][3]}\n")
        performance_metrics_totals = defaultdict(list)
        for algorithm in args.algorithms:
            performance_metrics_totals[algorithm] = {}
            for distinct_class in classes:
                performance_metrics_totals[algorithm][distinct_class] = [0 for zero in range(4)]
        for fold_metrics in performance_metrics_per_fold:
            for algorithm in args.algorithms:
                for distinct_class in classes:
                    for i in range(4):
                        performance_metrics_totals[algorithm][distinct_class][i] += fold_metrics[algorithm][distinct_class][i]

        performance_metrics_averages = defaultdict(list)
        for algorithm in args.algorithms:
            print(f" {algorithm} average results ".center(40, "-"))
            performance_metrics_averages[algorithm] = {}
            for i, distinct_class in enumerate(classes):
                performance_metrics_averages[algorithm][distinct_class] = []
                for performance_metrics_total in performance_metrics_totals[algorithm][distinct_class]:
                    performance_metrics_averages[algorithm][distinct_class].append(
                        performance_metrics_total / args.folds)

                print(f" {distinct_class} performance metrics ".center(40, "_"))
                print(f"accuracy = {performance_metrics_averages[algorithm][distinct_class][0]}")
                print(f"precision = {performance_metrics_averages[algorithm][distinct_class][1]}")
                print(f"recall = {performance_metrics_averages[algorithm][distinct_class][2]}")
                print(f"f1 score = {performance_metrics_averages[algorithm][distinct_class][3]}\n")

    else:
        raise Exception("Either trainingSplit or folds must be provided: training split to run the algorithm(s) with the"
                        " input training split or folds to run the algorithms using stratified k fold cross validation "
                        "with the input number of folds")
